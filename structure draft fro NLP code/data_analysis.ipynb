{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43,)\n",
      "(9,)\n",
      "(52,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>canonical_smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC(C)CN(Sc1ccc2c(c1)CCO2)[C@H](CO)CCCCNC(=O)[C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CC(C)CN(Sc1ccc2c(c1)OCCO2)[C@H](CO)CCCCNC(=O)[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CC(C)CN([C@H](CO)CCCCNC(=O)[C@H](Cc1ccccc1Br)N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cc1c(O)cccc1C(=O)N[C@@H](Cc1ccccc1Br)C(=O)NCCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CC(C)CN([C@H](CO)CCCCNC(=O)[C@H](Cc1ccccc1Br)N...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    canonical_smiles\n",
       "0  CC(C)CN(Sc1ccc2c(c1)CCO2)[C@H](CO)CCCCNC(=O)[C...\n",
       "1  CC(C)CN(Sc1ccc2c(c1)OCCO2)[C@H](CO)CCCCNC(=O)[...\n",
       "2  CC(C)CN([C@H](CO)CCCCNC(=O)[C@H](Cc1ccccc1Br)N...\n",
       "3  Cc1c(O)cccc1C(=O)N[C@@H](Cc1ccccc1Br)C(=O)NCCC...\n",
       "4  CC(C)CN([C@H](CO)CCCCNC(=O)[C@H](Cc1ccccc1Br)N..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "protease = pd.read_csv('C:\\\\Users\\\\Rebe\\\\Documents\\\\Python programms\\\\Capstone_code\\\\Data\\\\hiv_protease_inhibitors_data_chemlb.csv')\n",
    "protease = protease['canonical_smiles']\n",
    "print(protease.shape)\n",
    "\n",
    "nrti = pd.read_csv('C:\\\\Users\\\\Rebe\\\\Documents\\\\Python programms\\\\Capstone_code\\\\Data\\\\nrtis_hiv_chemlb_full.csv')\n",
    "nrti = nrti['canonical_smiles']\n",
    "print(nrti.shape)\n",
    "\n",
    "#Add protease and nrtis data together\n",
    "\n",
    "smile_3 = pd.concat([protease, nrti], axis=0)\n",
    "print(smile_3.shape)\n",
    "smile_3.head()\n",
    "smile_3.to_csv('smile_3.csv', index=False)\n",
    "\n",
    "hello = pd.read_csv('smile_3.csv')\n",
    "\n",
    "hello.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protease dataset shape: (43,)\n",
      "NRTI dataset shape: (9,)\n",
      "Average Tanimoto similarity per protease inhibitor to all NRTIs: 0.09415152618379889\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "import numpy as np\n",
    "\n",
    "# Load the datasets\n",
    "protease_path = 'C:\\\\Users\\\\Rebe\\\\Documents\\\\Python programms\\\\Capstone_code\\\\Data\\\\hiv_protease_inhibitors_data_chemlb.csv'\n",
    "nrti_path = 'C:\\\\Users\\\\Rebe\\\\Documents\\\\Python programms\\\\Capstone_code\\\\Data\\\\nrtis_hiv_chemlb_full.csv'\n",
    "\n",
    "protease_df = pd.read_csv(protease_path)['canonical_smiles']\n",
    "nrti_df = pd.read_csv(nrti_path)['canonical_smiles']\n",
    "\n",
    "print(f\"Protease dataset shape: {protease_df.shape}\")\n",
    "print(f\"NRTI dataset shape: {nrti_df.shape}\")\n",
    "\n",
    "# Generate RDKit Mol objects\n",
    "protease_mols = [Chem.MolFromSmiles(smiles) for smiles in protease_df if smiles is not None]\n",
    "nrti_mols = [Chem.MolFromSmiles(smiles) for smiles in nrti_df if smiles is not None]\n",
    "\n",
    "# Generate fingerprints\n",
    "protease_fps = [AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024) for mol in protease_mols]\n",
    "nrti_fps = [AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024) for mol in nrti_mols]\n",
    "\n",
    "# Calculate pairwise Tanimoto similarities\n",
    "# Note: This can be computationally intensive for large datasets\n",
    "similarity_matrix = []\n",
    "for protease_fp in protease_fps:\n",
    "    row_similarities = [DataStructs.FingerprintSimilarity(protease_fp, nrti_fp) for nrti_fp in nrti_fps]\n",
    "    similarity_matrix.append(row_similarities)\n",
    "\n",
    "# Convert similarity matrix to a numpy array for easier analysis\n",
    "similarity_array = np.array(similarity_matrix)\n",
    "\n",
    "# Example analysis: Average similarity of each protease inhibitor to all NRTIs\n",
    "average_similarity_per_protease = similarity_array.mean(axis=1)\n",
    "print(f\"Average Tanimoto similarity per protease inhibitor to all NRTIs: {average_similarity_per_protease.mean()}\")\n",
    "\n",
    "# This is a basic analysis. You might want to explore further, e.g., max or min similarities, or distribution analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical test\n",
    "\n",
    "def mannwhitney(descriptor, verbose=False):\n",
    "  # https://machinelearningmastery.com/nonparametric-statistical-significance-tests-in-python/\n",
    "  from numpy.random import seed\n",
    "  from numpy.random import randn\n",
    "  from scipy.stats import mannwhitneyu\n",
    "\n",
    "# seed the random number generator\n",
    "  seed(3)\n",
    "\n",
    "# actives and inactives\n",
    "  selection = [descriptor, 'type']\n",
    "  df = df_final[selection]\n",
    "  protease = df[df.type == 'protease']\n",
    "  protease = protease[descriptor]\n",
    "\n",
    "  selection = [descriptor, 'type']\n",
    "  df = df_final[selection]\n",
    "  nrti = df[df.type == 'nrti']\n",
    "  nrti = nrti[descriptor]\n",
    "\n",
    "# compare samples\n",
    "  stat, p = mannwhitneyu(protease, nrti)\n",
    "  #print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "# interpret\n",
    "  alpha = 0.05\n",
    "  if p > alpha:\n",
    "    interpretation = 'Same distribution (fail to reject H0)'\n",
    "  else:\n",
    "    interpretation = 'Different distribution (reject H0)'\n",
    "  \n",
    "  results = pd.DataFrame({'Descriptor':descriptor,\n",
    "                          'Statistics':stat,\n",
    "                          'p':p,\n",
    "                          'alpha':alpha,\n",
    "                          'Interpretation':interpretation}, index=[0])\n",
    "  filename = 'mannwhitneyu_' + descriptor + '.csv'\n",
    "  #results.to_csv(filename)\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modification to create the dataframe for the statistical analysis of all the descriptors\n",
    "\n",
    "\n",
    "def all_mannwhitney():  \n",
    "    from numpy.random import seed\n",
    "    from scipy.stats import mannwhitneyu\n",
    "    import pandas as pd  # Imported pandas for DataFrame operations\n",
    "\n",
    "    list_d = ['MW', 'LogP', 'hbd', 'hba', 'psa', 'num_ro5_violations', 'cx_logd', 'np_likeness_score', 'pIC50', 'mw_freebase', 'full_mwt',\n",
    "              'NumHAcceptors', 'NumHDonors']\n",
    "\n",
    "    # Initialize results DataFrame\n",
    "    results = pd.DataFrame(columns=['Descriptor', 'Statistics', 'p', 'alpha', 'Interpretation'])\n",
    "\n",
    "    # Seed the random number generator\n",
    "    seed(3)\n",
    "\n",
    "    for descriptor in list_d:\n",
    "        selection = [descriptor, 'type']\n",
    "        df = df_final[selection]\n",
    "        protease = df[df.type == 'protease'][descriptor]\n",
    "        nrti = df[df.type == 'nrti'][descriptor]\n",
    "\n",
    "        # Compare samples\n",
    "        stat, p = mannwhitneyu(protease, nrti)\n",
    "\n",
    "        # Interpret results\n",
    "        alpha = 0.05\n",
    "        if p > alpha:\n",
    "            interpretation = 'Same distribution (fail to reject H0)'\n",
    "        else:\n",
    "            interpretation = 'Different distribution (reject H0)'\n",
    "\n",
    "        temp_df = pd.DataFrame({'Descriptor': [descriptor],\n",
    "                                'Statistics': [stat],\n",
    "                                'p': [p],\n",
    "                                'alpha': [alpha],\n",
    "                                'Interpretation': [interpretation]})\n",
    "\n",
    "        results = pd.concat([results, temp_df], ignore_index=True)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mannwhitney()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "import logging\n",
    "import tempfile\n",
    "from collections import Counter\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles, CanonSmiles\n",
    "\n",
    "import deepchem as dc\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "import atomInSmiles as ais\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This vocab only focus in characters not in pairs.\n",
    "class CreateVocab:\n",
    "    def __init__(self, datasets):\n",
    "        \"\"\"\n",
    "        Initialize the CreateVocab object with multiple datasets.\n",
    "\n",
    "        :param datasets: List of datasets (strings) to build the vocabulary from.\n",
    "        \"\"\"\n",
    "        self.stoi = {}\n",
    "        self.itos = {}\n",
    "        self.build_vocab(datasets)\n",
    "\n",
    "    def build_vocab(self, datasets):\n",
    "        \"\"\"\n",
    "        Builds the vocabulary from the given datasets.\n",
    "\n",
    "        :param datasets: List of datasets (strings) to build the vocabulary from.\n",
    "        \"\"\"\n",
    "        all_text = ''.join(datasets)  # Concatenate all datasets\n",
    "        unique_chars = sorted(set(all_text))  # Extract unique characters\n",
    "\n",
    "        self.stoi = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "        self.itos = {idx: char for idx, char in enumerate(unique_chars)}\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encodes a given text into a list of indices.\n",
    "\n",
    "        :param text: String to be encoded.\n",
    "        :return: List of indices corresponding to the characters in the text.\n",
    "        \"\"\"\n",
    "        return [self.stoi[char] for char in text]\n",
    "\n",
    "    def decode(self, indices):\n",
    "        \"\"\"\n",
    "        Decodes a list of indices into a string.\n",
    "\n",
    "        :param indices: List of indices to be decoded.\n",
    "        :return: Decoded string.\n",
    "        \"\"\"\n",
    "        return ''.join(self.itos[idx] for idx in indices)\n",
    "\n",
    "\n",
    "def preprocess_smiles(smiles):\n",
    "    \"\"\"\n",
    "    Preprocesses a given SMILES string by adding padding and start and end tokens.\n",
    "\n",
    "    :param smiles: SMILES string to be preprocessed.\n",
    "    :return: Preprocessed SMILES string.\n",
    "    \"\"\"\n",
    "    text = [str(i) + \"*\" for i in smiles]\n",
    "    text = str(text)[1:-1]\n",
    "    return text\n",
    "\n",
    "# Load and process datasets\n",
    "smile_1 = pd.read_csv('smile_1.csv')['smiles']\n",
    "smile_2 = pd.read_csv('smile_2.csv')['smiles']\n",
    "smile_3 = pd.read_csv('smile_3.csv')['canonical_smiles']\n",
    "\n",
    "processed_smile_1 = preprocess_smiles(smile_1)\n",
    "processed_smile_2 = preprocess_smiles(smile_2)\n",
    "processed_smile_3 = preprocess_smiles(smile_3)\n",
    "\n",
    "# Combine processed datasets\n",
    "combined_text = processed_smile_1 + processed_smile_2 + processed_smile_3\n",
    "\n",
    "# Create vocabulary\n",
    "vocab = CreateVocab([combined_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = vocab.encode(text)\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encoded_text, dtype=torch.long)\n",
    "n = int(0.9 * len(data))  # 90% for training, rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create section vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create character vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = vocab.encode(text)\n",
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encoded_text, dtype=torch.long)\n",
    "n = int(0.9 * len(data))  # 90% for training, rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 350\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 100\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "vocab_size = len(vocab.stoi)\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigramLanguageModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_path = \"2_BWC.pkl\"\n",
    "model.load_state_dict(torch.load(model_state_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "smiles_1 = pd.read_csv('smile_3.csv')\n",
    "#smiles_1\n",
    "smiles_1 = smiles_1['canonical_smiles']\n",
    "#smiles_1\n",
    "\n",
    "\n",
    "text = [str(i) + \"*\" for i in smiles_1]\n",
    "\n",
    "text = str(text)[1:-1]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = vocab.decode(m.generate(context, max_new_tokens=2000)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = \"\".join(val)\n",
    "new = new.split('*')\n",
    "\n",
    "new = [s for s in new if len(s) > 2]\n",
    "\n",
    "new = [s.replace(' ', '') for s in new]\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "\n",
    "mols = [Chem.MolFromSmiles(smi) for smi in new if Chem.MolFromSmiles(smi) is not None]\n",
    "Draw.MolsToGridImage(mols, molsPerRow=5, subImgSize=(200, 200), legends=[str(i) for i in range(len(mols))])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
