{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors, Crippen\n",
    "from rdkit.Chem.Draw import MolsToGridImage\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from rdkit import Chem, RDLogger\n",
    "from rdkit.Chem import BondType\n",
    "from rdkit.Chem.Draw import MolsToGridImage\n",
    "\n",
    "RDLogger.DisableLog(\"rdApp.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>smiles</th>\n",
       "      <th>qed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CC(C)(C)c1ccc2occ(CC(=O)Nc3ccccc3F)c2c1\\n</td>\n",
       "      <td>0.731901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C[C@@H]1CC(Nc2cncc(-c3nncn3C)c2)C[C@@H](C)C1\\n</td>\n",
       "      <td>0.941112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N#Cc1ccc(-c2ccc(O[C@@H](C(=O)N3CCCC3)c3ccccc3)...</td>\n",
       "      <td>0.626105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCOC(=O)[C@@H]1CCCN(C(=O)c2nc(-c3ccc(C)cc3)n3c...</td>\n",
       "      <td>0.716225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N#CC1=C(SCC(=O)Nc2cccc(Cl)c2)N=C([O-])[C@H](C#...</td>\n",
       "      <td>0.809572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              smiles       qed\n",
       "0          CC(C)(C)c1ccc2occ(CC(=O)Nc3ccccc3F)c2c1\\n  0.731901\n",
       "1     C[C@@H]1CC(Nc2cncc(-c3nncn3C)c2)C[C@@H](C)C1\\n  0.941112\n",
       "2  N#Cc1ccc(-c2ccc(O[C@@H](C(=O)N3CCCC3)c3ccccc3)...  0.626105\n",
       "3  CCOC(=O)[C@@H]1CCCN(C(=O)c2nc(-c3ccc(C)cc3)n3c...  0.716225\n",
       "4  N#CC1=C(SCC(=O)Nc2cccc(Cl)c2)N=C([O-])[C@H](C#...  0.809572"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = pd.read_csv('smiles1_qed.csv')\n",
    "d2 = pd.read_csv('smiles2_qed.csv')\n",
    "d3 = pd.read_csv('smiles3_qed.csv')\n",
    "\n",
    "frames = [d1, d2, d3]\n",
    "df = pd.concat(frames)\n",
    "\n",
    "df = df.drop_duplicates(subset=['smiles'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMILE_CHARSET = '[\"C\", \"B\", \"F\", \"I\", \"H\", \"O\", \"N\", \"S\", \"P\", \"Cl\", \"Br\"]'\n",
    "\n",
    "bond_mapping = {\"SINGLE\": 0, \"DOUBLE\": 1, \"TRIPLE\": 2, \"AROMATIC\": 3}\n",
    "bond_mapping.update(\n",
    "    {0: BondType.SINGLE, 1: BondType.DOUBLE, 2: BondType.TRIPLE, 3: BondType.AROMATIC}\n",
    ")\n",
    "SMILE_CHARSET = ast.literal_eval(SMILE_CHARSET)\n",
    "\n",
    "MAX_MOLSIZE = max(df[\"smiles\"].str.len())\n",
    "SMILE_to_index = dict((c, i) for i, c in enumerate(SMILE_CHARSET))\n",
    "index_to_SMILE = dict((i, c) for i, c in enumerate(SMILE_CHARSET))\n",
    "atom_mapping = dict(SMILE_to_index)\n",
    "atom_mapping.update(index_to_SMILE)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 100\n",
    "\n",
    "VAE_LR = 5e-4\n",
    "NUM_ATOMS = 120  # Maximum number of atoms\n",
    "\n",
    "ATOM_DIM = len(SMILE_CHARSET)  # Number of atom types\n",
    "BOND_DIM = 4 + 1  # Number of bond types\n",
    "LATENT_DIM = 435  # Size of the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_graph(smiles):\n",
    "    # Converts SMILES to molecule object\n",
    "    molecule = Chem.MolFromSmiles(smiles)\n",
    "\n",
    "    # Initialize adjacency and feature tensor\n",
    "    adjacency = np.zeros((BOND_DIM, NUM_ATOMS, NUM_ATOMS), \"float32\")\n",
    "    features = np.zeros((NUM_ATOMS, ATOM_DIM), \"float32\")\n",
    "\n",
    "    # loop over each atom in molecule\n",
    "    for atom in molecule.GetAtoms():\n",
    "        i = atom.GetIdx()\n",
    "        atom_type = atom_mapping[atom.GetSymbol()]\n",
    "        features[i] = np.eye(ATOM_DIM)[atom_type]\n",
    "        # loop over one-hop neighbors\n",
    "        for neighbor in atom.GetNeighbors():\n",
    "            j = neighbor.GetIdx()\n",
    "            bond = molecule.GetBondBetweenAtoms(i, j)\n",
    "            bond_type_idx = bond_mapping[bond.GetBondType().name]\n",
    "            adjacency[bond_type_idx, [i, j], [j, i]] = 1\n",
    "\n",
    "    # Where no bond, add 1 to last channel (indicating \"non-bond\")\n",
    "    # Notice: channels-first\n",
    "    adjacency[-1, np.sum(adjacency, axis=0) == 0] = 1\n",
    "\n",
    "    # Where no atom, add 1 to last column (indicating \"non-atom\")\n",
    "    features[np.where(np.sum(features, axis=1) == 0)[0], -1] = 1\n",
    "\n",
    "    return adjacency, features\n",
    "\n",
    "\n",
    "def graph_to_molecule(graph):\n",
    "    # Unpack graph\n",
    "    adjacency, features = graph\n",
    "\n",
    "    # RWMol is a molecule object intended to be edited\n",
    "    molecule = Chem.RWMol()\n",
    "\n",
    "    # Remove \"no atoms\" & atoms with no bonds\n",
    "    keep_idx = np.where(\n",
    "        (np.argmax(features, axis=1) != ATOM_DIM - 1)\n",
    "        & (np.sum(adjacency[:-1], axis=(0, 1)) != 0)\n",
    "    )[0]\n",
    "    features = features[keep_idx]\n",
    "    adjacency = adjacency[:, keep_idx, :][:, :, keep_idx]\n",
    "\n",
    "    # Add atoms to molecule\n",
    "    for atom_type_idx in np.argmax(features, axis=1):\n",
    "        atom = Chem.Atom(atom_mapping[atom_type_idx])\n",
    "        _ = molecule.AddAtom(atom)\n",
    "\n",
    "    # Add bonds between atoms in molecule; based on the upper triangles\n",
    "    # of the [symmetric] adjacency tensor\n",
    "    (bonds_ij, atoms_i, atoms_j) = np.where(np.triu(adjacency) == 1)\n",
    "    for (bond_ij, atom_i, atom_j) in zip(bonds_ij, atoms_i, atoms_j):\n",
    "        if atom_i == atom_j or bond_ij == BOND_DIM - 1:\n",
    "            continue\n",
    "        bond_type = bond_mapping[bond_ij]\n",
    "        molecule.AddBond(int(atom_i), int(atom_j), bond_type)\n",
    "\n",
    "    # Sanitize the molecule; for more information on sanitization, see\n",
    "    # https://www.rdkit.org/docs/RDKit_Book.html#molecular-sanitization\n",
    "    flag = Chem.SanitizeMol(molecule, catchErrors=True)\n",
    "    # Let's be strict. If sanitization fails, return None\n",
    "    if flag != Chem.SanitizeFlags.SANITIZE_NONE:\n",
    "        return None\n",
    "\n",
    "    return molecule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = d3\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df = data.sample(frac=0.75, random_state=42)\n",
    "val_df = data.drop(train_df.index)\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "train_df = data.sample(frac=0.75, random_state=42)  # random state is a seed value\n",
    "train_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MoleculeDataset(Dataset):\n",
    "    def __init__(self, df, smiles_to_graph):\n",
    "        self.adjacency_tensors = []\n",
    "        self.feature_tensors = []\n",
    "        self.qed_tensors = []\n",
    "\n",
    "        for idx in range(len(df)):\n",
    "            adjacency, features = smiles_to_graph(df.iloc[idx]['smiles'])\n",
    "            qed = df.iloc[idx]['qed']\n",
    "            self.adjacency_tensors.append(torch.tensor(adjacency, dtype=torch.float32))\n",
    "            self.feature_tensors.append(torch.tensor(features, dtype=torch.float32))\n",
    "            self.qed_tensors.append(torch.tensor([qed], dtype=torch.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.qed_tensors)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.adjacency_tensors[idx], self.feature_tensors[idx], self.qed_tensors[idx]\n",
    "\n",
    "# Example usage\n",
    "train_dataset = MoleculeDataset(train_df, smiles_to_graph)\n",
    "val_dataset = MoleculeDataset(val_df, smiles_to_graph)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class RelationalGraphConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, use_bias=False, activation=F.relu):\n",
    "        super(RelationalGraphConvLayer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_bias = use_bias\n",
    "        self.activation = activation\n",
    "        \n",
    "        # Initialize the weights\n",
    "        self.kernel = nn.Parameter(torch.Tensor(in_channels, out_channels))\n",
    "        if use_bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_channels))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    #kernel act as the weights \n",
    "    def reset_parameters(self):\n",
    "        # Initialize parameters similarly to 'glorot_uniform' in Keras\n",
    "        nn.init.xavier_uniform_(self.kernel)\n",
    "        if self.use_bias:\n",
    "            nn.init.zeros_(self.bias)\n",
    "    \n",
    "    def forward(self, adjacency, features):\n",
    "        # Assuming 'adjacency' is [batch_size, bond_dim, num_atoms, num_atoms]\n",
    "        # and 'features' is [batch_size, num_atoms, atom_dim]\n",
    "        # We want to aggregate information from neighbors, apply linear transformation, and then activation.\n",
    "        \n",
    "        # Aggregate neighbor features\n",
    "        # Note: Depending on the exact format of your adjacency matrix, you may need to adjust this operation.\n",
    "        support = torch.matmul(adjacency, features.unsqueeze(1))  # [batch_size, bond_dim, num_atoms, atom_dim]\n",
    "        \n",
    "        # Apply linear transformation\n",
    "        output = torch.einsum('bnad,df->bnaf', support, self.kernel)  # [batch_size, bond_dim, num_atoms, out_channels]\n",
    "        \n",
    "        if self.use_bias:\n",
    "            output += self.bias\n",
    "        \n",
    "        # Reduce bond types dimension\n",
    "        output = torch.sum(output, dim=1)  # [batch_size, num_atoms, out_channels]\n",
    "        \n",
    "        # Apply non-linear transformation\n",
    "        if self.activation is not None:\n",
    "            output = self.activation(output)\n",
    "        \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_adjacency, val_features, val_qed = [], [], []\n",
    "for idx in range(12):\n",
    "    v_adjacency, v_features = smiles_to_graph(val_df.iloc[idx][\"smiles\"])\n",
    "    v_qed = val_df.iloc[idx][\"qed\"]\n",
    "    val_adjacency.append(torch.tensor(v_adjacency, dtype=torch.float32))\n",
    "    val_features.append(torch.tensor(v_features, dtype=torch.float32))\n",
    "    val_qed.append(torch.tensor(v_qed, dtype=torch.float32))\n",
    "\n",
    "# Convert lists to PyTorch tensors\n",
    "val_adjacency = torch.stack(val_adjacency)\n",
    "val_features = torch.stack(val_features)\n",
    "val_qed = torch.stack(val_qed).unsqueeze(-1)  # Adding an extra dime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GlobalAveragePooling1D(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return torch.mean(x, dim=1)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, gconv_units, latent_dim, feature_shape, dense_units, dropout_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.gconv_layers = nn.ModuleList([RelationalGraphConvLayer(feature_shape[1], gconv_units[0])])\n",
    "        for i in range(1, len(gconv_units)):\n",
    "            self.gconv_layers.append(RelationalGraphConvLayer(gconv_units[i-1], gconv_units[i]))\n",
    "        self.global_avg_pool = GlobalAveragePooling1D()\n",
    "        self.dense_layers = nn.ModuleList([nn.Linear(gconv_units[-1], dense_units[0])])\n",
    "        for i in range(1, len(dense_units)):\n",
    "            self.dense_layers.append(nn.Linear(dense_units[i-1], dense_units[i]))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.z_mean = nn.Linear(dense_units[-1], latent_dim)\n",
    "        self.log_var = nn.Linear(dense_units[-1], latent_dim)\n",
    "    \n",
    "    def forward(self, adjacency, features):\n",
    "        x = features\n",
    "        for gconv in self.gconv_layers:\n",
    "            x = gconv(adjacency, x)\n",
    "        x = self.global_avg_pool(x)\n",
    "        for dense in self.dense_layers:\n",
    "            x = F.relu(dense(x))\n",
    "            x = self.dropout(x)\n",
    "        z_mean = self.z_mean(x)\n",
    "        log_var = self.log_var(x)\n",
    "        return z_mean, log_var\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, adjacency_shape, feature_shape, dense_units, dropout_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dense_layers = nn.ModuleList([nn.Linear(latent_dim, dense_units[0])])\n",
    "        for i in range(1, len(dense_units)):\n",
    "            self.dense_layers.append(nn.Linear(dense_units[i-1], dense_units[i]))\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.adjacency_shape = adjacency_shape\n",
    "        self.feature_shape = feature_shape\n",
    "        self.adj_decoder = nn.Linear(dense_units[-1], np.prod(adjacency_shape))\n",
    "        self.feature_decoder = nn.Linear(dense_units[-1], np.prod(feature_shape))\n",
    "    \n",
    "    def forward(self, z):\n",
    "        x = z\n",
    "        for dense in self.dense_layers:\n",
    "            x = F.relu(dense(x))  # Changed to ReLU for consistency\n",
    "            x = self.dropout(x)\n",
    "        x_adj = self.adj_decoder(x).view(-1, *self.adjacency_shape)\n",
    "        x_features = self.feature_decoder(x).view(-1, *self.feature_shape)\n",
    "        return torch.softmax(x_adj, dim=1), torch.softmax(x_features, dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Sampling(nn.Module):\n",
    "    def forward(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch, dim = z_log_var.size()\n",
    "        epsilon = torch.randn_like(z_log_var)  # Generates a tensor of random numbers from a normal distribution with the same size as z_log_var\n",
    "        return z_mean + torch.exp(0.5 * z_log_var) * epsilon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "class MoleculeGenerator(nn.Module):\n",
    "    def __init__(self, encoder, decoder, max_len):\n",
    "        super(MoleculeGenerator, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.property_prediction_layer = nn.Linear(max_len, 1)  # Assuming max_len aligns with the expected input size for property prediction\n",
    "        \n",
    "    def forward(self, adjacency_tensor, feature_tensor):\n",
    "        z_mean, z_log_var = self.encoder(adjacency_tensor, feature_tensor)\n",
    "        z = self._reparameterize(z_mean, z_log_var)\n",
    "        gen_adjacency, gen_features = self.decoder(z)\n",
    "        qed_pred = self.property_prediction_layer(z)  # Example usage, may need adjustment\n",
    "        return gen_adjacency, gen_features, qed_pred, z_mean, z_log_var\n",
    "\n",
    "    def _reparameterize(self, z_mean, z_log_var):\n",
    "        std = torch.exp(0.5*z_log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return z_mean + eps*std\n",
    "    \n",
    "    # Custom loss function to be defined based on your needs\n",
    "    def loss_function(self, *args, **kwargs):\n",
    "        # Implement loss calculation\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, epochs=10):\n",
    "    optimizer = Adam(model.parameters())\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for adjacency_tensor, feature_tensor, qed_tensor in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            gen_adjacency, gen_features, qed_pred, z_mean, z_log_var = model(adjacency_tensor, feature_tensor)\n",
    "            loss = model.loss_function(gen_adjacency, gen_features, qed_tensor, qed_pred, z_mean, z_log_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Implement validation step\n",
    "        # Update your loss tracking variables here\n",
    "\n",
    "# Assuming `train_loader` and `val_loader` are PyTorch DataLoader instances containing your training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(reconstructed_adjacency, reconstructed_features, true_adjacency, true_features, z_mean, z_log_var, qed_pred, true_qed):\n",
    "    # Reconstruction loss\n",
    "    recon_loss_adj = F.binary_cross_entropy(reconstructed_adjacency, true_adjacency, reduction='sum')\n",
    "    recon_loss_feat = F.binary_cross_entropy(reconstructed_features, true_features, reduction='sum')\n",
    "    \n",
    "    # Property prediction loss (if applicable)\n",
    "    qed_loss = F.mse_loss(qed_pred, true_qed)\n",
    "    \n",
    "    # KL divergence\n",
    "    kl_loss = -0.5 * torch.sum(1 + z_log_var - z_mean.pow(2) - z_log_var.exp())\n",
    "    \n",
    "    return recon_loss_adj + recon_loss_feat + kl_loss + qed_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m], Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Model instantiation\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m encoder \u001b[38;5;241m=\u001b[39m \u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgconv_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLATENT_DIM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNUM_ATOMS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mATOM_DIM\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(latent_dim\u001b[38;5;241m=\u001b[39mLATENT_DIM, adjacency_shape\u001b[38;5;241m=\u001b[39m(BOND_DIM, NUM_ATOMS, NUM_ATOMS), feature_shape\u001b[38;5;241m=\u001b[39m(NUM_ATOMS, ATOM_DIM), dense_units\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m512\u001b[39m], dropout_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     25\u001b[0m model \u001b[38;5;241m=\u001b[39m MoleculeGenerator(encoder, decoder, MAX_MOLSIZE)\n",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[1;34m(self, gconv_units, latent_dim, feature_shape, dense_units, dropout_rate)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, gconv_units, latent_dim, feature_shape, dense_units, dropout_rate):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28msuper\u001b[39m(Encoder, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgconv_layers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([RelationalGraphConvLayer(\u001b[43mfeature_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m, gconv_units[\u001b[38;5;241m0\u001b[39m])])\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(gconv_units)):\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgconv_layers\u001b[38;5;241m.\u001b[39mappend(RelationalGraphConvLayer(gconv_units[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], gconv_units[i]))\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "def train_vae(model, train_loader, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for data in train_loader:\n",
    "            adjacency_tensor, feature_tensor, qed_tensor = data\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            gen_adjacency, gen_features, qed_pred, z_mean, z_log_var = model(adjacency_tensor, feature_tensor)\n",
    "            \n",
    "            # Loss computation\n",
    "            loss = vae_loss(gen_adjacency, gen_features, adjacency_tensor, feature_tensor, z_mean, z_log_var, qed_pred, qed_tensor)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Model instantiation\n",
    "encoder = Encoder(gconv_units=[9], latent_dim=LATENT_DIM, feature_shape=(NUM_ATOMS, ATOM_DIM), dense_units=[512], dropout_rate=0.0)\n",
    "decoder = Decoder(latent_dim=LATENT_DIM, adjacency_shape=(BOND_DIM, NUM_ATOMS, NUM_ATOMS), feature_shape=(NUM_ATOMS, ATOM_DIM), dense_units=[128, 256, 512], dropout_rate=0.2)\n",
    "model = MoleculeGenerator(encoder, decoder, MAX_MOLSIZE)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=VAE_LR)\n",
    "\n",
    "# Assuming `train_loader` is defined\n",
    "train_vae(model, train_loader, optimizer, epochs=EPOCHS)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
